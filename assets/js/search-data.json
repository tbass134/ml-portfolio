{
  
    
        "post0": {
            "title": "Machine Learing Skills Gathering",
            "content": "When looking for a new job, in a new field, its hard to know exactly what skills a company is looking for. So I wanted to know what are the most freaquently used keywords for Machine Learning Engineer job descriptions. . One way to do this, would be go though every job description and find the skills that are listed, then write them down. . However, being a software engineer, I decided to automate the process. So for this project, I decided to use web scraping to find the latest job descriptions and pull the keywords . Overview . For this project, I first needed to find a way to get the latest job posting by title and location. I&#39;ve been using BeautifulSoup for awhile and knew that if I could load a job description, I could then use BeautifuSoup to read that job description. . pip install wordcloud . from bs4 import BeautifulSoup import requests import re from collections import Counter import nltk nltk.download(&#39;stopwords&#39;) from nltk.corpus import stopwords import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) import pandas as pd %matplotlib inline . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Package stopwords is already up-to-date! . Return the unique words in the job descirpotion. . The get_data function in the code cell above loads the webpage containing a job description. I then find the actual job description text by grabbing the jobsearch-jobDescriptionText id in the webpage. . To grab the most import words in the job description, i&#39;m using NER(Named Entity Reconition) in Spacy to return only phrases that contain a Noun . This returns a much better list of keywords instead of just returning all the words. . def get_data(website): print(&quot;loading job data from&quot;,website) site = requests.get(website).content soup = BeautifulSoup(site) job = soup.find(&quot;div&quot;, {&quot;class&quot;: &quot;jobsearch-jobDescriptionText&quot;}) text = [] for p in job.find_all(&quot;li&quot;): text.append(p.text) text = &quot; &quot;.join(text) return text def get_keywords(text): result = [] pos_tag = [&#39;NOUN&#39;] doc = nlp(text.lower()) for chunk in doc.noun_chunks: final_chunk = &quot;&quot; for token in chunk: if (token.pos_ in pos_tag): final_chunk = final_chunk + token.text + &quot; &quot; if final_chunk: result.append(final_chunk.strip()) for token in doc: if (token.text in nlp.Defaults.stop_words or token.text in punctuation): continue if (token.pos_ in pos_tag): result.append(token.text) results = list(set(result)) return [word for word in results if len(word) &gt; 5] . Next, let&#39;s grab a random job description to see the first 20 keywords . text = get_data(&quot;https://www.indeed.com/viewjob?jk=14d6693d03cd1d67&amp;from=serp&amp;vjs=3&quot;) get_keywords(text)[:20] . loading job data from https://www.indeed.com/viewjob?jk=14d6693d03cd1d67&amp;from=serp&amp;vjs=3 . [&#39;modeling&#39;, &#39;product metrics&#39;, &#39;benchmark&#39;, &#39;product needs&#39;, &#39;engineers&#39;, &#39;quality&#39;, &#39;reviews&#39;, &#39;product understanding&#39;, &#39;recommendation problems&#39;, &#39;data structures&#39;, &#39;code reviews&#39;, &#39;production quality solutions&#39;, &#39;intuition&#39;, &#39;performance&#39;, &#39;problems&#39;, &#39;benchmark metrics&#39;, &#39;options&#39;, &#39;engineering&#39;, &#39;multitude&#39;, &#39;pattern&#39;] . Here, I grab a Machine Learning Engineer position for Twitter and return a list of keywords. This just finds the job description, saves the text as a list of words and removes words that are not revelated(and, the etc). These words are also know as stopwords. . def get_job_page(city, state, job): final_city = city.split() final_city = &#39;+&#39;.join(word for word in final_city) final_site_list = [&#39;http://www.indeed.com/jobs?q=&#39;, job, &#39;&amp;l=&#39;, final_city, &#39;%2C+&#39;, state] final_site = &#39;&#39;.join(final_site_list) final_site html = requests.get(final_site).content soup = BeautifulSoup(html) num_jobs_area = soup.find(&quot;div&quot;,{&quot;id&quot;:&#39;searchCountPages&#39;}).text num_jobs_area job_numbers = re.findall(&quot; d+&quot;,num_jobs_area) current_page, total_num_pages = int(job_numbers[0]), int(job_numbers[1]) return final_site, current_page, total_num_pages final_site, current_page, total_num_pages = get_job_page(&quot;New York&quot;, &quot;NY&quot;, &quot;&#39;machine-learning-engineer&#39;&quot;) . In the cell above, I first need to get the first page of results. The format of the url needs to be: . &#39;http://www.indeed.com/jobs?q=&#39;, job, &#39;&amp;l=&#39;, final_city, &#39;%2C+&#39;, state, %start=0 . So for Machine Learning Engineer positions located in New York, NY, the URL will look like this: . https://www.indeed.com/jobs?q=machine-learning-engineer&amp;l=New+York,+NY&amp;start=0 . After I get the first page, I then need to grab the total number of pages. This is located in the searchCountPages div. Next, I pull out the 2 numbers in that div, which looks like . Page x of y . I used a simple regex to pull the 2 numbers from this div and used this as the start page(x) and total number of pages(y) This is saved in current_page and total_num_pages. . In the next cell, I then loop thought the number of pages and append the start index to url as the index of the current page. . I first calcuate the number of pages by . num_pages = round(total_num_pages/15) . This takes the total number and divides by 15. This is because I dont know exactly how many results are returned for each page, and after some testing, 15 is about the average. . At the time of this post, there are about 47 jobs posted. And therefore, I need to load 3 pages of results.. . Next, I used BeautifuSoup to load of the content of each page and find the job url, which is used to fetch the job description. The url to the job contains the path clk. Therefore, I needed to filter the url&#39;s returned from the page to only contain urls that have the clk path. The results returned from the filter function are the job description urls. . Finally, for each URL, I then get the job description words using the get_data() function from ealier . def get_jobs(final_site, current_page, total_num_pages): doc_frequency = Counter() num_pages = round(total_num_pages/15) print(&quot;Number of Pages&quot;,num_pages) for i in range(num_pages): print(&#39;Getting page&#39;, i) start_num = str(i*10) current_page = &#39;&#39;.join([final_site, &#39;&amp;start=&#39;, start_num]) print(&quot;total pages&quot;,current_page) page = requests.get(current_page).content soup = BeautifulSoup(page) job_link_area = soup.find(id = &#39;resultsCol&#39;) job_URLS = [&quot;http://www.indeed.com&quot; + a[&#39;href&#39;] for a in job_link_area.find_all(&#39;a&#39;, href=True)] job_URLS = filter(lambda x:&#39;clk&#39; in x, job_URLS) for url in job_URLS: job_desc_text = get_data(url) keywords = get_keywords(job_desc_text) job_descriptions.append(keywords) [doc_frequency.update(item) for item in job_descriptions] return doc_frequency doc_frequency = get_jobs(final_site, current_page, total_num_pages) . Number of Pages 3 Getting page 0 total pages http://www.indeed.com/jobs?q=&#39;machine-learning-engineer&#39;&amp;l=New+York%2C+NY&amp;start=0 loading job data from http://www.indeed.com/rc/clk?jk=14d6693d03cd1d67&amp;fccid=7a3824693ee1074b&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=7dab88fcf921f518&amp;fccid=e2a37b55a93b50b2&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=2596a581cc5517bf&amp;fccid=2689b06a1fbc0bc4&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=c4d2b45f58a26a83&amp;fccid=5bd99dfa21c8a490&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=6436e44787661e89&amp;fccid=1b3df0faac85255e&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=1a9c4e5ad5cb4393&amp;fccid=ce3b6f8ba0332e43&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=c17bcc51c3d895e8&amp;fccid=16a97ed26c75bf2d&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=4cae4a1a1a7b3381&amp;fccid=a2bc1e38165c6c18&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=eb7a384f353a5b2b&amp;fccid=fa632e5431d07ea8&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=fcc6d006aab7c18f&amp;fccid=837edc3ec6c2367e&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=dfdc4bffb02b1bfd&amp;fccid=fe2d21eef233e94a&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=ba04472204b14351&amp;fccid=7a3824693ee1074b&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=b72d920e7684a780&amp;fccid=2973259ddc967948&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=a56a341d36871b8e&amp;fccid=f057e04c37cca134&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=c88e1dfd6bb6562d&amp;fccid=bf8d0f5b45d49fcf&amp;vjs=3 . So for each job, I append the words to a doc_frequency Counter, which just counts the total number of words contained in every job description. . To Close this out, lets plot these keywords into a WordCloud for better visibility . from wordcloud import WordCloud import matplotlib.pyplot as plt wordcloud = WordCloud(width=1000, height=400) wordcloud.generate_from_frequencies(frequencies=doc_frequency) plt.figure(figsize=(20,10) ) plt.imshow(wordcloud) plt.axis(&quot;off&quot;) plt.show() . For reference, lets also plot the top 100 words: . doc_frequency.most_common()[:100] . [(&#39;experience&#39;, 722), (&#39;learning&#39;, 705), (&#39;machine&#39;, 677), (&#39;models&#39;, 559), (&#39;production&#39;, 553), (&#39;systems&#39;, 479), (&#39;software&#39;, 470), (&#39;environment&#39;, 430), (&#39;development&#39;, 430), (&#39;engineering&#39;, 429), (&#39;ability&#39;, 415), (&#39;science&#39;, 408), (&#39;computer&#39;, 388), (&#39;skills&#39;, 380), (&#39;business&#39;, 376), (&#39;techniques&#39;, 365), (&#39;solutions&#39;, 349), (&#39;machine learning&#39;, 338), (&#39;understanding&#39;, 327), (&#39;pipelines&#39;, 326), (&#39;problems&#39;, 297), (&#39;computer science&#39;, 290), (&#39;performance&#39;, 283), (&#39;product&#39;, 278), (&#39;analysis&#39;, 277), (&#39;algorithms&#39;, 275), (&#39;knowledge&#39;, 272), (&#39;quality&#39;, 270), (&#39;engineers&#39;, 266), (&#39;infrastructure&#39;, 237), (&#39;features&#39;, 237), (&#39;training&#39;, 231), (&#39;languages&#39;, 226), (&#39;opportunities&#39;, 222), (&#39;communication&#39;, 222), (&#39;degree&#39;, 217), (&#39;technologies&#39;, 212), (&#39;design&#39;, 211), (&#39;variety&#39;, 208), (&#39;stakeholders&#39;, 202), (&#39;recommendation&#39;, 201), (&#39;programming&#39;, 196), (&#39;impact&#39;, 192), (&#39;tensorflow&#39;, 189), (&#39;technology&#39;, 184), (&#39;projects&#39;, 184), (&#39;industry&#39;, 183), (&#39;language&#39;, 182), (&#39;pytorch&#39;, 179), (&#39;processing&#39;, 179), (&#39;building&#39;, 177), (&#39;platform&#39;, 177), (&#39;applications&#39;, 176), (&#39;modeling&#39;, 175), (&#39;healthcare&#39;, 174), (&#39;data pipelines&#39;, 173), (&#39;testing&#39;, 172), (&#39;frameworks&#39;, 172), (&#39;research&#39;, 171), (&#39;concepts&#39;, 166), (&#39;intelligence&#39;, 166), (&#39;structures&#39;, 165), (&#39;analytics&#39;, 157), (&#39;company&#39;, 157), (&#39;data structures&#39;, 156), (&#39;decision&#39;, 154), (&#39;practices&#39;, 153), (&#39;processes&#39;, 153), (&#39;requirements&#39;, 150), (&#39;methods&#39;, 148), (&#39;sources&#39;, 146), (&#39;background&#39;, 146), (&#39;platforms&#39;, 146), (&#39;communication skills&#39;, 146), (&#39;mining&#39;, 145), (&#39;insights&#39;, 143), (&#39;health&#39;, 139), (&#39;people&#39;, 138), (&#39;statistics&#39;, 137), (&#39;challenges&#39;, 136), (&#39;communities&#39;, 135), (&#39;employees&#39;, 135), (&#39;deployment&#39;, 131), (&#39;record&#39;, 130), (&#39;mission&#39;, 129), (&#39;improvements&#39;, 128), (&#39;reliability&#39;, 125), (&#39;culture&#39;, 125), (&#39;scientists&#39;, 124), (&#39;support&#39;, 123), (&#39;startup&#39;, 121), (&#39;information&#39;, 120), (&#39;detection&#39;, 118), (&#39;operations&#39;, 114), (&#39;services&#39;, 114), (&#39;interest&#39;, 114), (&#39;work experience&#39;, 113), (&#39;vision&#39;, 113), (&#39;data science&#39;, 113), (&#39;mathematics&#39;, 112)] . Other Postions . For funzies, lets see what other skills are most present in other roles, day Data Scientist . final_site, current_page, total_num_pages = get_job_page(&quot;New York&quot;, &quot;NY&quot;, &quot;data+scientist&quot;) doc_frequency = get_jobs(final_site, current_page, total_num_pages) wordcloud = WordCloud(width=1000, height=400) wordcloud.generate_from_frequencies(frequencies=doc_frequency) plt.figure(figsize=(20,10) ) plt.imshow(wordcloud) plt.axis(&quot;off&quot;) plt.show() . Number of Pages 43 Getting page 0 total pages http://www.indeed.com/jobs?q=data+scientist&amp;l=New+York%2C+NY&amp;start=0 loading job data from http://www.indeed.com/rc/clk?jk=b6e4fc84c7d476ea&amp;fccid=1b50fcfb150b1b48&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=62d25207253030d0&amp;fccid=049a66f3ffa9366a&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=3483ac001da233ed&amp;fccid=e571ceda55e25d27&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=549b343d185ed525&amp;fccid=30cb52ad6dd37131&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=00b8c73d3919724e&amp;fccid=30cb52ad6dd37131&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=65702776ee7f6924&amp;fccid=be3b11aa573faee7&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=b80c4e9ec85f3c6d&amp;fccid=978d9fd9799d55a8&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=148f2e5e7a652368&amp;fccid=2721c1c2ba33c638&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=de01cecba45a129c&amp;fccid=b374f2a780e04789&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=84fe5a346eb0847c&amp;fccid=5c9de1aedf73e3b6&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=18f4e428f8530bf0&amp;fccid=dce16d3eba100f4b&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=1636faa0308fd08f&amp;fccid=1577085fc2290983&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=676fa6b5a127e015&amp;fccid=5c9de1aedf73e3b6&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=63a117535b33b4a1&amp;fccid=1b50fcfb150b1b48&amp;vjs=3 loading job data from http://www.indeed.com/rc/clk?jk=29e58def9b023a1a&amp;fccid=5bcd1ef0a7f4fb99&amp;vjs=3 . Conclusion . So far, we have a nice application that shows the most frequent keywords in Machine Learning Engineer and Data Scientist postions. . I think there are more things that can be done with this, especially, using Named Enity Reconition for returning the keywords. One though is to see if Google&#39;s BERT pretrained model would be better at extracting skills. We could also try to simpliy and give a list of keywords(&quot;python&quot;, &quot;pytorch&quot; .. etc) and see how many times those skills are listened. .",
            "url": "https://tonyhung.xyz/jupyter/beautifulsoup/2020/11/16/_11_17_Job_Keywords.html",
            "relUrl": "/jupyter/beautifulsoup/2020/11/16/_11_17_Job_Keywords.html",
            "date": " • Nov 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a Machine Learning model to detect spam in SMS",
            "content": "In this notebook, we&#39;ll show how to build a simple machine learning model to predict that a SMS is spam or not. . The notebook was built to go along with my talk in May 2020 for Vonage Developer Day . youtube: https://www.youtube.com/watch?v=5d4_HpMLXf4&amp;t=1s . We&#39;ll be using the scikit-learn library to train a model on a set of messages which are labeled as spam and non spam(aka ham) messages. . After our model is trained, we&#39;ll deploy to an AWS Lambda in which its input will be a message, and its output will be the prediction(spam or ham). . Before we build a model, we&#39;ll need some data. So we&#39;ll use the SMS Spam Collection DataSet. . This dataset contains over 5k messages which are labeled spam or ham. In the following cell, we&#39;ll download the dataset . !wget --no-check-certificate https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip !unzip /content/smsspamcollection.zip . --2020-07-22 00:49:18-- https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252 Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 203415 (199K) [application/x-httpd-php] Saving to: ‘smsspamcollection.zip’ smsspamcollection.z 100%[===================&gt;] 198.65K 509KB/s in 0.4s 2020-07-22 00:49:19 (509 KB/s) - ‘smsspamcollection.zip’ saved [203415/203415] Archive: /content/smsspamcollection.zip inflating: SMSSpamCollection inflating: readme . Once we have downloaded the datatset, we&#39;ll load into a Pandas Dataframe and view the first 10 rows of the dataset. . import pandas as pd df = pd.read_csv(&quot;/content/SMSSpamCollection&quot;, sep=&#39; t&#39;, header=None, names=[&#39;label&#39;, &#39;message&#39;]) df.head() . label message . 0 ham | Go until jurong point, crazy.. Available only ... | . 1 ham | Ok lar... Joking wif u oni... | . 2 spam | Free entry in 2 a wkly comp to win FA Cup fina... | . 3 ham | U dun say so early hor... U c already then say... | . 4 ham | Nah I don&#39;t think he goes to usf, he lives aro... | . Next, we need to first understand the data before building a model. . We&#39;ll first need to see how many messages are considered spam or ham . df.label.value_counts() . ham 4825 spam 747 Name: label, dtype: int64 . From the cell above, we see that 4825 messages are valid messages, and only 747 messages are labled as spam. . Lets now just view some messages that are ham and some that are spam . spam = df[df[&quot;label&quot;] == &quot;spam&quot;] spam.head() . label message . 2 spam | Free entry in 2 a wkly comp to win FA Cup fina... | . 5 spam | FreeMsg Hey there darling it&#39;s been 3 week&#39;s n... | . 8 spam | WINNER!! As a valued network customer you have... | . 9 spam | Had your mobile 11 months or more? U R entitle... | . 11 spam | SIX chances to win CASH! From 100 to 20,000 po... | . ham = df[df[&quot;label&quot;] == &quot;ham&quot;] ham.head() . label message . 0 ham | Go until jurong point, crazy.. Available only ... | . 1 ham | Ok lar... Joking wif u oni... | . 3 ham | U dun say so early hor... U c already then say... | . 4 ham | Nah I don&#39;t think he goes to usf, he lives aro... | . 6 ham | Even my brother is not like to speak with me. ... | . after looking at some messages that spam and ham, we can see the spam messages look spammy.. . Preprocessing . The next step is to get the dataset ready to build a model. A machine learning model can only deal with numbers, so we&#39;ll have to convert our text into numbers using TfidfVectorizer . TfidfVectorizer converts a collection of raw documents to a matrix of term frequency-inverse document frequency features. Also known as TF-IDF. . In our case, a document is each message. For each message, we&#39;ll compute the number of times a term is in our document divied by all the terms in the document times the total number of documents divded by the number of documents that contain the specific term . source . The output will be a matrix in which the rows will be all the terms, and the colums will be all the documents . This notebook by Mike Bernico by goes into more detail on TF-IDF and how to calucate without using sklearn. . first, we&#39;ll split the dataset into a train and test set. For the training set, we&#39;ll take 80% of the data from the dataset, and use that for training the model. The rest of the dataset(20%) will be used for testing the model. . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(df[&#39;message&#39;], df[&#39;label&#39;], test_size = 0.2, random_state = 1) . once we split our data, we can use the TfidfVectorizer. This will return a sparse matrix(a matrix with mostly 0&#39;s) . from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() X_train = vectorizer.fit_transform(X_train) X_test = vectorizer.transform(X_test) . After we fit the TfidfVectorizer to the sentenes, lets plot the matrix as a pandas dataframe to understand what TfidfVectorizer is doing . feature_names = vectorizer.get_feature_names() tfid_df = pd.DataFrame(tfs.T.todense(), index=feature_names) print(tfid_df[1200:1205]) . 0 1 2 3 4 ... 4452 4453 4454 4455 4456 backdoor 0.0 0.0 0.0 0.000000 0.0 ... 0.0 0.0 0.0 0.0 0.0 backwards 0.0 0.0 0.0 0.000000 0.0 ... 0.0 0.0 0.0 0.0 0.0 bad 0.0 0.0 0.0 0.193352 0.0 ... 0.0 0.0 0.0 0.0 0.0 badass 0.0 0.0 0.0 0.000000 0.0 ... 0.0 0.0 0.0 0.0 0.0 badly 0.0 0.0 0.0 0.000000 0.0 ... 0.0 0.0 0.0 0.0 0.0 [5 rows x 4457 columns] . From the table above, each word in our dataset are the rows are the sentenes index are the columns. We&#39;ve only plotted a few rows in the middle of the dataframe for a better understanding of the data. . Next, we&#39;ll train a model using Gaussian Naive Bayes in scikit-learn. Its a good starting algorithm for text classification. We&#39;ll then print out the accuracy of the model by using the training set and our confusion_matrix . Model Training . To train our model, we&#39;ll use A Navie Bayes algorhtymn to train our model . The formula for Navie Bayes is: $$ P(S|W) = P(W|S) times P(S) over P(W|S) times P(S) + P(W|H) times P(h) $$. . P(s|w) - The probability(P) of a message is spam(s) Given(|) a word(w) . = . P(w|s) - probability(P) that a word(w) is spam(s) . * . P(s) - Overall probability(P) that ANY message is spam(s) . / . P(w|s) - probability(P) that a word(w) exists in spam messages(s) . * . P(s) - Overall probability(P) that ANY message is spam(s) . + . P(w|h) - probability(P) the word(w) appears in non-spam(h) messages . * . P(h) - Overall probability(P) that any message is not-spam(h) . from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix clf = GaussianNB() clf.fit(X_train.toarray(),y_train) . GaussianNB(priors=None, var_smoothing=1e-09) . y_true, y_pred = y_test, clf.predict(X_test.toarray()) accuracy_score(y_true, y_pred) . 0.8986547085201794 . print(classification_report(y_true, y_pred)) . precision recall f1-score support ham 0.99 0.89 0.94 968 spam 0.57 0.93 0.71 147 accuracy 0.90 1115 macro avg 0.78 0.91 0.82 1115 weighted avg 0.93 0.90 0.91 1115 . cmtx = pd.DataFrame( confusion_matrix(y_true, y_pred, labels=[&#39;ham&#39;, &#39;spam&#39;]), index=[&#39;ham&#39;, &#39;spam&#39;], columns=[&#39;ham&#39;, &#39;spam&#39;] ) print(cmtx) . ham spam ham 866 102 spam 11 136 . Grid Search . from sklearn.model_selection import GridSearchCV parameters = {&quot;var_smoothing&quot;:[1e-9, 1e-5, 1e-1]} gs_clf = GridSearchCV( GaussianNB(), parameters) gs_clf.fit(X_train.toarray(),y_train) . GridSearchCV(cv=None, error_score=nan, estimator=GaussianNB(priors=None, var_smoothing=1e-09), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;var_smoothing&#39;: [1e-09, 1e-05, 0.1]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . gs_clf.best_params_ . {&#39;var_smoothing&#39;: 0.1} . y_true, y_pred = y_test, gs_clf.predict(X_test.toarray()) accuracy_score(y_true, y_pred) . 0.9650224215246637 . cmtx = pd.DataFrame( confusion_matrix(y_true, y_pred, labels=[&#39;ham&#39;, &#39;spam&#39;]), index=[&#39;ham&#39;, &#39;spam&#39;], columns=[&#39;ham&#39;, &#39;spam&#39;] ) print(cmtx) . ham spam ham 932 36 spam 3 144 . print(classification_report(y_true, y_pred)) . precision recall f1-score support ham 1.00 0.96 0.98 968 spam 0.80 0.98 0.88 147 accuracy 0.97 1115 macro avg 0.90 0.97 0.93 1115 weighted avg 0.97 0.97 0.97 1115 . From our trained model, we get about 96% accuracy. Which is pretty good. . We also print out the confusion_matrix. This shows how many messages were classificed correctly. In the first column and first row, we see that 866 messages that were classified as ham were actaully ham and 136 messages that were predicted as spam, were in fact spam. . Next, lets test our model with some examples messages . Inference . message = vectorizer.transform([&quot;i&#39;m on my way home&quot;]) message = message.toarray() gs_clf.predict(message) . array([&#39;ham&#39;], dtype=&#39;&lt;U4&#39;) . message = vectorizer.transform([&quot;this offer is to good to be true&quot;]) message = message.toarray() gs_clf.predict(message) . array([&#39;spam&#39;], dtype=&#39;&lt;U4&#39;) . The final step is the save the model and the tf-idf vectorizer. We will use these when clasifing incoming messages on our lambda function . import joblib joblib.dump(gs_clf, &quot;model.pkl&quot;) joblib.dump(vectorizer, &quot;vectorizer.pkl&quot;) . [&#39;vectorizer.pkl&#39;] . Lambda . Once our model is trained, we&#39;ll now put it in a production envioroment. . For this example, we&#39;ll create a lambda function to host our model. . The lambda function will be attached to an API gateway in which we&#39;ll be able to have a endpoint to make our predictions . Deploying a scikit-learn model to lambda isnt as easy as you would think. You can&#39;t just import your libraries, espcially scikit-learn to work. . Here&#39;s what we&#39;ll need to do in order to deploy our model . Spin up EC2 instance | SSH into the instance and install our dependencies | copy the lambda function code from this repo | Run a bash script that zips up the : | zip the code, including the packages | upload to S3 | point the lambda function to to s3 file | . Create an EC2 instance . If you have an aws account: . Go to EC2 on the console and click Launch Instance. | Select the first available AMI(Amazon Linux 2 AMI). | Select the t2.micro instance, then click Review and Launch | Click the Next button | Under IAM Role, Click Create New Role | Create a new role with the following policies: AmazonS3FullAccess AWSLambdaFullAccess Name your role and click create role | Under permissions, create a new role that has access to the following: | lambda full access | S3 full access | . These will be needed when uploading our code to your S3 bucket and pointing the lambda function to zip file that will be creating later. . Create a new private key pair and click Lanuch Instance | Note, in order to use the key, you have to run chmod 400 on the key when downloaded to your local machine. | . After the instance spins up, you&#39;ll need to connect to it via ssh . Find the newly created instance on EC2 and click Connect | On your local machine, navigate to terminal and run the the command from the Example. It will look something like:ssh -i &quot;{PATH TO KEY}&quot; {user_name}@ec2-{instance_ip}.compute-1.amazonaws.com . | . Install packages . Before installing packages, you will need to install python and pip. You can follow the steps here These will most likey be: . sudo yum install python37 curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user verify pip is installed using bash pip --version . You will also need to install git . sudo yum install git -y . When connected to the instance, clone the repo . git clone https://github.com/tbass134/SMS-Spam-Classifier-lambda . This repo contains everything we need to make predictions. These includes the pickle files from the model and vectorizer, as well as the lambda function to make predictions and returns its response cd into the SMS-Spam-Classifier-lambda/lambda folder . Next, you you will need to install the sklearn library. | On your instance, type: pip install -t . sklearn This will import the library into its own folder | . Next, if you want to use your trained model, it will need to be uploaded into your ec2 instance. If your using Google Colab, navigate to the files tab, right click on my_model.pk and vectorizer.pkl and click download. Note, the sample repo already contains a trained model so this is optional. . To upload your trained model, you can use a few ways: . Fork the repo, add your models, and checkout on the ec2 instance You can use scp to copy to files from your local machine to the instance To upload the model file we saved . scp -i {PATH_TO_KEY} vectorizer.pkl ec2-user@{INSTANCE_NAME}: . and we&#39;ll do the same for the model . scp -i {PATH_TO_KEY} my_model.pkl ec2-user@{INSTANCE_NAME}: . | . The other method is to upload the files to s3 and have your lambda function load the files from there using Boto . def load_s3_file(key): obj = s3.Object(MODEL_BUCKET, key) body = obj.get()[&#39;Body&#39;].read() return joblib.load(BytesIO(body)) model = load_s3_file({PATH_TO_S3_MODEL} vectorizer = load_s3_file({PATH_TO_S3_VECTORIZER} . | . Create lambda function . On the AWS console, navigate to https://console.aws.amazon.com/lambda | Click on the Create function button | Make sure Author from scratch is selected | Name your function | Set the runtime to Python 3.7 | Under Execution Role, create a new role with basic permissions | Click Create Function | . Create S3 bucket . In order to push our code to a lambda function, we need to first copy zip up the code and libraies to a S3 bucket. From here, our lambda function will load the zip file from this bucket. . On the AWS console under Services, Search for S3 | Click Create Bucket | Name your bucket, and click Create Bucket at the bottom of the page. | . Upload to lambda . Next, we&#39;ll run the publish.shscript inside the root of the repo, which does the following: . zip up the pacakages, including our Python code, model and transformer. | upload the zip to an S3 bucket | point our lambda function to this bucket | . when calling this script, we need to pass in 3 arguments: . The name of the zip file. We can call it zip.zip for now | The name of the S3 bucket that we will upload the zip to | the name of lambda function bash publish.sh {ZIP_FILE_NAME} {S3_BUCKET} {LAMBDA_FUNCTION_NAME} . | . If everything is successful, your lambda function will be deployed. If you see errors, make sure your EC2 instance has a IAM role that has an S3 permission, and Lambda permissions. See this guide for more info. . Add HTTP endpoint . The final piece will be to add a API gateway. On the configuration tab on the lambda function . click Add Trigger | Click on the select a trigger box and select API Gateway | Click on Create an API | Set API Type to REST API | Set Security to OPEN (make sure to secure when deploying for production) | At the bottom, click Add | . For detail, see this documentation . We can now test the endpoint by using curl and making a call to our endpoint. Under API Gateway section in lambda, click on oi . In the lambda function, we are looking for the message GET parameter. When we make our request, we&#39;ll pass a query parameter called message. This will contain the string we want to make a prediction on. . ham_message = &quot;im on my way home&quot;.replace(&quot; &quot;, &quot;%20&quot;) ham_message . %%bash -s &quot;$ham_message&quot; curl --location --request GET &quot;https://e18fmcospk.execute-api.us-east-1.amazonaws.com/default/spam-detection?message=$1&quot; . {&#34;prediction&#34;: &#34;ham&#34;} . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 21 100 21 0 0 10 0 0:00:02 0:00:02 --:--:-- 10 . spam_message = &quot;this offer is to good to be true&quot;.replace(&quot; &quot;, &quot;%20&quot;) spam_message . %%bash -s &quot;$spam_message&quot; curl --location --request GET &quot;https://e18fmcospk.execute-api.us-east-1.amazonaws.com/default/spam-detection?message=$1&quot; . {&#34;prediction&#34;: &#34;spam&#34;} . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 22 100 22 0 0 13 0 0:00:01 0:00:01 --:--:-- 13 . Google Cloud Functions . For non-amazon users, we can use Google Cloud Functions to deploy our model for use in our Vonage SMS API app Code is here . Create Vonage SMS Application . The final step is to build a Vonage SMS Application. Have a look at this blog post on how to build yourself Our application will receive an SMS https://developer.nexmo.com/messaging/sms/code-snippets/receiving-an-sms . and will send a SMS back to the user with its prediction https://developer.nexmo.com/messaging/sms/code-snippets/send-an-sms . . To work through this example, you will need the following . Login / Signup to Vonage SMS API | Rent a phone number | Assign a publicly accessable url via ngrok to that phone number | . We&#39;ll also build a simple Flask app that will make a request to our API Gateway . git clone https://github.com/tbass134/SMS-Spam-Classifier-lambda.git cd app . Next we&#39;ll create a virtual environment and install the requirements using pip . virtualenv venv --python=python3 source venv/bin/activate pip install -r requirments.txt . Next, create a .env file with the following: . NEXMO_API_KEY={YOUR_NEXMO_API_KEY} NEXMO_API_SECRET={YOUR_NEXMO_API_SECRET} NEXMO_NUMBER={YOUR_NEXMO_NUMBER API_GATEWAY_URL={FULL_API_GATEWAY} . Finally, you can run the application: . python app.py . This will spin up a webserver listening on PORT 3000 . Fin .",
            "url": "https://tonyhung.xyz/jupyter/2020/11/06/SMS_Spam_Classifier_Demo.html",
            "relUrl": "/jupyter/2020/11/06/SMS_Spam_Classifier_Demo.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Bob Ross Episode Text Generator",
            "content": "This project shows how we can gather data and build a model to generate text in the style of bob ross. In order to gather data, we&#39;ll be using a script called download-yt-playlist.py that uses the YouTube API to download a Bob Ross playlist. This playlist contains most of the Bob Ross epiodes as well as the transcript from each epiode . !pip install beautifulsoup4 . Collecting beautifulsoup4 Using cached https://files.pythonhosted.org/packages/f8/c7/741c97d7366f4779ca73d244904978b43a81fd37d85fcf05ad19d472c1ce/beautifulsoup4-4.6.3-py2-none-any.whl Installing collected packages: beautifulsoup4 Successfully installed beautifulsoup4-4.6.3 . import pandas as pd import tensorflow as tf from bs4 import BeautifulSoup import numpy as np . Next, we&#39;ll import the dataset that we created using the download-ty-playlist script, The csv is included in the repo . we&#39;ll then load the dataset into a pandas dataframe Our csv contains 249 rows, which are the number of episodes that was returned by the script. We&#39;ve removed any columns that are empty, since not all of the episodes had a transcript . df = pd.read_csv(&#39;bob_ross/bob_ross_episodes.csv&#39;, index_col=0, parse_dates=[&#39;snippet.publishedAt&#39;], usecols=[&#39;snippet.description&#39;, &#39;snippet.publishedAt&#39;, &#39;snippet.title&#39;, &#39;transcript&#39;]) df.dropna(inplace=True) # df[&#39;snippet.publishedAt&#39;] =pd.to_datetime(df[&#39;snippet.publishedAt&#39;]) df.sort_values(by=&#39;snippet.publishedAt&#39;, inplace=True) df.head() . snippet.publishedAt snippet.title transcript . snippet.description . Season 21 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Valley View, Tranquil Dawn, Royal Majesty, Serenity, Cabin at Trails End, Mountain Rhapsody, Wilderness Cabin, By the Sea, Indian Summer, Blue Winter, Desert Glow, Lone Mountain, and Florida’s Glory. n nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe n nSeason 21 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi5_UcEWkQZu23WzQP1Tkxq3 n nThe Joy of Painting Season 20 is now on iTunes! - http://bit.ly/iTunesBobRoss n nOfficial Bob Ross website - http://www.BobRoss.com n nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss n nAll episodes of Bob Ross are now live on Roku - http://bit.ly/BobRossOnRoku n nOriginally aired on 9/5/1990 2015-03-25 16:32:35 | Bob Ross - Valley View (Season 21 Episode 1) | (bright music) - Hello, I&amp;#39;m Bob Ross and n... | . Season 6 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Blue River, Nature&#39;s Edge, Morning Mist, Whispering Stream, Secluded Forest, Snow Trail, Arctic Beauty, Horizons West, High Chateau, Country Life, Western Expanse, Marshlands, and Blaze of Color. n nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe n nSeason 6 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi5UR35RJsvL0Xvlm3oeY4Ma n nThe Joy of Painting : Season 20 is now on iTunes! http://bit.ly/iTunesBobRoss n nOfficial Bob Ross website - http://www.BobRoss.com n nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss 2015-03-27 17:01:20 | Bob Ross - Arctic Beauty (Season 6 Episode 7) | - Welcome back. Awful glad you could join me t... | . Season 6 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Blue River, Nature&#39;s Edge, Morning Mist, Whispering Stream, Secluded Forest, Snow Trail, Arctic Beauty, Horizons West, High Chateau, Country Life, Western Expanse, Marshlands, and Blaze of Color. n nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe n nSeason 6 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi5UR35RJsvL0Xvlm3oeY4Ma n nThe Joy of Painting : Season 20 is now on iTunes! http://bit.ly/iTunesBobRoss n nOfficial Bob Ross website - http://www.BobRoss.com n nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss 2015-03-27 17:24:24 | Bob Ross - Horizons West (Season 6 Episode 8) | - Welcome back, I&amp;#39;m awful nglad to see you... | . Season 6 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Blue River, Nature&#39;s Edge, Morning Mist, Whispering Stream, Secluded Forest, Snow Trail, Arctic Beauty, Horizons West, High Chateau, Country Life, Western Expanse, Marshlands, and Blaze of Color. n nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe n nSeason 6 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi5UR35RJsvL0Xvlm3oeY4Ma n nThe Joy of Painting : Season 20 is now on iTunes! http://bit.ly/iTunesBobRoss n nOfficial Bob Ross website - http://www.BobRoss.com n nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss 2015-03-27 18:16:39 | Bob Ross - Blue River (Season 6 Episode 1) | (peaceful instrumental music) - Hello, I&amp;#39;m... | . Season 5 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Mountain Waterfall, Twilight Meadow, Mountain Blossoms, Winter Stillness, Quiet Pond, OCean Sunrise, Bubbling Brook, Arizona Splendor, Anatomy of a Wave, The Windmill, Autumn Glory, Indian Girl, and Meadow Stream. n nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe n nSeason 5 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi6bAFRfcqgpKP4T4SnoxoAz n nThe Joy of Painting : Season 20 is now on iTunes! http://bit.ly/iTunesBobRoss n nOfficial Bob Ross website - http://www.BobRoss.com n nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss 2015-03-27 18:34:49 | Bob Ross - Twilight Meadow (Season 5 Episode 2) | - Hi, welcome back. I&amp;#39;m glad to see you to... | . The following will build out text generator. We&#39;ll do the following, . load a sample of the dataset (about 30%)&quot;, | combine all the transcription into one long string, | We use BeautifulSoup to remove any html tags in the text, | we&#39;ll then generate a list of all the characters in the transcription | . test_df = df.sample(frac=.3) . len(test_df) . 75 . descriptions = &#39;&#39; all_transcriptions = &#39;&#39; for index, row in test_df.iterrows(): all_transcriptions += BeautifulSoup(row[&#39;transcript&#39;],&quot;lxml&quot;).get_text().replace(&#39; n&#39;, &#39; &#39;) . len(all_transcriptions) . 1522162 . Next, we&#39;ll just display a piece of the all_transcriptions just to see what it looks like . all_transcriptions[:100] . &#34;- Hi, welcome back. I&#39;m certainly glad you could join us today. And, as you can see, today I have on&#34; . chars = sorted(list(set(all_transcriptions))) print(&#39;Count of unique characters (i.e., features):&#39;, len(chars)) char_indices = dict((c, i) for i, c in enumerate(chars)) indices_char = dict((i, c) for i, c in enumerate(chars)) . Count of unique characters (i.e., features): 81 . Next, we&#39;ll generate seperate lists of all the strings that we&#39;ll feed into the model This list is 40 charcters of the full text, seperated by 3 characters(step) . maxlen = 40 step = 3 sentences = [] next_chars = [] for i in range(0, len(all_transcriptions) - maxlen, step): sentences.append(all_transcriptions[i: i + maxlen]) next_chars.append(all_transcriptions[i + maxlen]) print(&#39;nb sequences:&#39;, len(sentences)) print(sentences[:10], &quot; n&quot;) print(next_chars[:10]) . nb sequences: 507374 [&#34;- Hi, welcome back. I&#39;m certainly glad y&#34;, &#34;i, welcome back. I&#39;m certainly glad you &#34;, &#34;welcome back. I&#39;m certainly glad you cou&#34;, &#34;come back. I&#39;m certainly glad you could &#34;, &#34;e back. I&#39;m certainly glad you could joi&#34;, &#34;ack. I&#39;m certainly glad you could join u&#34;, &#34;. I&#39;m certainly glad you could join us t&#34;, &#34;&#39;m certainly glad you could join us toda&#34;, &#39;certainly glad you could join us today. &#39;, &#39;tainly glad you could join us today. And&#39;] [&#39;o&#39;, &#39;c&#39;, &#39;l&#39;, &#39;j&#39;, &#39;n&#39;, &#39;s&#39;, &#39;o&#39;, &#39;y&#39;, &#39;A&#39;, &#39;,&#39;] . We now have 507374 lists, that each contain 40 characters of the string, The first list is - Hi, welcome back. I&#39;m certainly glad y, followed by i, welcome back. I&#39;m certainly glad you . Next, we&#39;ll create tensors of x and y, that contain the lists of all the sentences, we&#39;ve created . x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) y = np.zeros((len(sentences), len(chars)), dtype=np.bool) for i, sentence in enumerate(sentences): for t, char in enumerate(sentence): x[i, t, char_indices[char]] = 1 y[i, char_indices[next_chars[i]]] = 1 . Builing The Model . Next, we&#39;ll build out our model . from keras.models import Sequential from keras.layers import Dense, Activation from keras.layers import LSTM from keras.optimizers import RMSprop from keras.callbacks import LambdaCallback, ModelCheckpoint import random import sys import io . The following are 2 functions that will print the prediction from each epoch, as well as the temperature . temperature is defined as the following: . &quot;Temperature is a scaling factor applied to the outputs of our dense layer before applying the softmaxactivation function. In a nutshell, it defines how conservative or creative the model&#39;s guesses are for the next character in a sequence. Lower values of temperature (e.g., 0.2) will generate &quot;safe &quot; guesses whereas values of temperature above 1.0 will start to generate riskier guesses. Think of it as the amount of surpise you&#39;d have at seeing an English word start with &quot;st &quot; versus &quot;sg &quot;. When temperature is low, we may get lots of the&#39;s and and&#39;s; when temperature is high, things get more unpredictable. . -- https://medium.freecodecamp.org/applied-introduction-to-lstms-for-text-generation-380158b29fb3 . def sample(preds, temperature=1.0): # helper function to sample an index from a probability array preds = np.asarray(preds).astype(&#39;float64&#39;) preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) def on_epoch_end(epoch, logs): # Function invoked for specified epochs. Prints generated text. # Using epoch+1 to be consistent with the training epochs printed by Keras if epoch+1 == 1 or epoch+1 == 15: print() print(&#39;-- Generating text after Epoch: %d&#39; % epoch) start_index = random.randint(0, len(all_transcriptions) - maxlen - 1) for diversity in [0.2, 0.5, 1.0, 1.2]: print(&#39;-- diversity:&#39;, diversity) generated = &#39;&#39; sentence = all_transcriptions[start_index: start_index + maxlen] generated += sentence print(&#39;-- Generating with seed: &quot;&#39; + sentence + &#39;&quot;&#39;) sys.stdout.write(generated) for i in range(400): x_pred = np.zeros((1, maxlen, len(chars))) for t, char in enumerate(sentence): x_pred[0, t, char_indices[char]] = 1. preds = model.predict(x_pred, verbose=0)[0] next_index = sample(preds, diversity) next_char = indices_char[next_index] generated += next_char sentence = sentence[1:] + next_char sys.stdout.write(next_char) sys.stdout.flush() print() else: print() print(&#39;-- Not generating text after Epoch: %d&#39; % epoch) generate_text = LambdaCallback(on_epoch_end=on_epoch_end) . def build_basic_model() model = Sequential() model.add(LSTM(batch_size, input_shape=(maxlen,len(chars)))) model.add(Dense(len(chars))) model.add(Activation(&quot;softmax&quot;)) return model . Here, we&#39;ll create our model. After a few tests, i&#39;ve seen that having 2 LSTMs with a batch size of 256, returns very good results. The first model is a basic model with 1 LSTM&#39; . batch_size=128 learning_rate = 0.01 model = build_basic_model() optimizer = RMSprop(lr=learning_rate) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=optimizer) # define the checkpoint filepath = &quot;weights.hdf5&quot; checkpoint = ModelCheckpoint(filepath, monitor=&#39;loss&#39;, verbose=1, save_best_only=True, mode=&#39;min&#39;) # fit model using our gpu with tf.device(&#39;/gpu:0&#39;): model.fit(x, y, batch_size=batch_size, epochs=15, verbose=1, callbacks=[generate_text, checkpoint]) . Epoch 1/15 - 168s - loss: 1.4472 -- Generating text after Epoch: 0 -- diversity: 0.2 -- Generating with seed: &#34;ainted in your mountain. There. Isn&#39;t th&#34; ainted in your mountain. There. Isn&#39;t the back, and there we go the to start out of the back. There we go with the back, and there. There we go. There. There. There. There. And we&#39;ll go back and start the background of this one of the back of the base of the back. There. And we&#39;ll start one of the brush on the back, and there&#39;s a little bit of the back. There we go that one of the back. There we go the back. And we want the back, and th -- diversity: 0.5 -- Generating with seed: &#34;ainted in your mountain. There. Isn&#39;t th&#34; ainted in your mountain. There. Isn&#39;t that easy like that. So let&#39;s do that. So let&#39;s go the back. Where we got the way of the old bright over the touching. I have to make the let the hang here. I stay over there, and that&#39;s run on the base the ingict of right in here lives right there. Okay, let&#39;s just go back and so back and just tap a little bright on our we want that with the lings on the old bit of the backer. Maybe they&#39;re wark it -- diversity: 1.0 -- Generating with seed: &#34;ainted in your mountain. There. Isn&#39;t th&#34; ainted in your mountain. There. Isn&#39;t them, ay up a many, back come spung it up ther hal chane and illoss, grettle strong, let&#39;s haf hereon well refcacalions and we just do clims to you&#39;re leanto so wet. There. There&#39;s and here and that easy. Hever in that&#39;s good let that little magic you. They&#39;s a little &#39;bounder There. There. And we use a littled, down like right here. And we can darker this tell. Don&#39;t kich xop and darked. Now the up -- diversity: 1.2 -- Generating with seed: &#34;ainted in your mountain. There. Isn&#39;t th&#34; ainted in your mountain. There. Isn&#39;t that you one DaneDrly. Othat&#39;els, bat that time. Let&#39;s do kap lives loanince wherever we just bring that lives rignt down here. Okayh over this paintings in ther staday shader, you, if a dirdiciodd then only shapes oir all right. There, they&#39;re nice that big,. E. There we goess like the painting and a little paintings. Changen and beons that you&#39;res. So nothill shay, fan brost, he ndime sortmort pea Epoch 00001: loss improved from inf to 1.44717, saving model to weights.hdf5 Epoch 2/15 - 165s - loss: 1.1462 -- Not generating text after Epoch: 1 Epoch 00002: loss improved from 1.44717 to 1.14621, saving model to weights.hdf5 Epoch 3/15 - 165s - loss: 1.0918 -- Not generating text after Epoch: 2 Epoch 00003: loss improved from 1.14621 to 1.09180, saving model to weights.hdf5 Epoch 4/15 - 164s - loss: 1.0639 -- Not generating text after Epoch: 3 Epoch 00004: loss improved from 1.09180 to 1.06386, saving model to weights.hdf5 Epoch 5/15 - 166s - loss: 1.0443 -- Not generating text after Epoch: 4 Epoch 00005: loss improved from 1.06386 to 1.04430, saving model to weights.hdf5 Epoch 6/15 - 165s - loss: 1.0308 -- Not generating text after Epoch: 5 Epoch 00006: loss improved from 1.04430 to 1.03085, saving model to weights.hdf5 Epoch 7/15 - 165s - loss: 1.0200 -- Not generating text after Epoch: 6 Epoch 00007: loss improved from 1.03085 to 1.02002, saving model to weights.hdf5 Epoch 8/15 - 163s - loss: 1.0105 -- Not generating text after Epoch: 7 Epoch 00008: loss improved from 1.02002 to 1.01048, saving model to weights.hdf5 Epoch 9/15 - 165s - loss: 1.0039 -- Not generating text after Epoch: 8 Epoch 00009: loss improved from 1.01048 to 1.00394, saving model to weights.hdf5 Epoch 10/15 - 167s - loss: 0.9996 -- Not generating text after Epoch: 9 Epoch 00010: loss improved from 1.00394 to 0.99955, saving model to weights.hdf5 Epoch 11/15 - 162s - loss: 0.9939 -- Not generating text after Epoch: 10 Epoch 00011: loss improved from 0.99955 to 0.99389, saving model to weights.hdf5 Epoch 12/15 - 166s - loss: 0.9918 -- Not generating text after Epoch: 11 Epoch 00012: loss improved from 0.99389 to 0.99178, saving model to weights.hdf5 Epoch 13/15 - 166s - loss: 0.9900 -- Not generating text after Epoch: 12 Epoch 00013: loss improved from 0.99178 to 0.98998, saving model to weights.hdf5 Epoch 14/15 - 165s - loss: 0.9873 -- Not generating text after Epoch: 13 Epoch 00014: loss improved from 0.98998 to 0.98731, saving model to weights.hdf5 Epoch 15/15 - 165s - loss: 0.9886 -- Generating text after Epoch: 14 -- diversity: 0.2 -- Generating with seed: &#34;tly tap this and pull down at the same t&#34; tly tap this and pull down at the same tore of the brush and the secret. And the . /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log after removing the cwd from sys.path. . y&#39;re back here in the bright and that lives right there. There we go. And it&#39;s all the color in the bristles that live in the bright and little bit of the bright red of the brush. And they come right there. And they&#39;re a little bit of the brown and some little things that live out here. And a little bit of the bright and then we have a little bit of the bit -- diversity: 0.5 -- Generating with seed: &#34;tly tap this and pull down at the same t&#34; tly tap this and pull down at the same thing because that&#39;s where a little bit of color in there. And we&#39;ll go sid it to get into the top, and they come right there. That still the black gesso basic little black, they paint thinner, right over the brush and happening beautiful way of your painting, that live out there. And becommen it back and sort of some big tree that lives real and sort of this side it dark to dry the big mountches a -- diversity: 1.0 -- Generating with seed: &#34;tly tap this and pull down at the same t&#34; tly tap this and pull down at the same toret it. Now then? I wanna have another little trees very shows here and there it&#39;ll put very all sort of decision it. Okay, and you&#39;s hadmy nice little, glad you want over here. They looks like dark afthainctoujbly aftosistic oary, maybe right in here, but that&#39;s goingtilly to dry pressort of the oh, that&#39;s sone&#39;s. , just to diffue all the would becamina&#39;s at sion. Let&#39;s start over here on cere a -- diversity: 1.2 -- Generating with seed: &#34;tly tap this and pull down at the same t&#34; tly tap this and pull down at the same twig, sup. some little grassy area, little workent, you see, it is gings. Don&#39;t neem comrstece a. Inlistion, makes this just in tood a reasinn forward. oDanatided just a knife. is us how a little foackes using aapriaens. I think we see letta after we very, it&#39;s undistant green something. It ... as phintione into a phthalo, eve all right forward, uh it, tllay right over, . Just bring ae. Anothe. O Epoch 00015: loss did not improve from 0.98731 . You can see that the results were good, but lets go deeper . Builing a better model . Here, we&#39;ll be using 2 LSTM&#39;s and dropout, durning training, we&#39;ll save the best model for later . from keras.layers import Dropout batch_size=256 learning_rate = 0.01 def build_deeper_model(): model = Sequential() model.add(LSTM(batch_size, input_shape=(maxlen, len(chars)), return_sequences=True)) model.add(Dropout(0.2)) model.add(LSTM(batch_size)) model.add(Dropout(0.2)) model.add(Dense(len(chars), activation=&#39;softmax&#39;)) model = build_deeper_model() model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;) # define the checkpoint filepath = &quot;bob_ross/weights-deepeer.hdf5&quot; checkpoint = ModelCheckpoint(filepath, monitor=&#39;loss&#39;, verbose=1, save_best_only=True, mode=&#39;min&#39;) # fit model using our gpu with tf.device(&#39;/gpu:0&#39;): model.fit(x, y, batch_size=64, epochs=15, verbose=1, callbacks=[generate_text, checkpoint]) . Loading the Model . After training, which took about 2 hours to train, using a GCP instance with a Tesla P100 GPU, we load the best model and perfrom a prediction . We loaded our model from our weights, and now we can predict I choose a temperature of 0.5. it seemed the have the best results . from keras.models import load_model model = load_model(&quot;bob_ross/weights-deepeer.hdf5&quot;) model # model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;) . &lt;keras.engine.sequential.Sequential at 0x7f78bb4f7748&gt; . model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;) . int_to_char = dict((i, c) for i, c in enumerate(chars)) . start_index = 0 for diversity in [0.5]: print(&#39;-- diversity:&#39;, diversity) generated = &#39;&#39; sentence = all_transcriptions[start_index: start_index + maxlen] generated += sentence # print(&#39;-- Generating with seed: &quot;&#39; + sentence + &#39;&quot;&#39;) sys.stdout.write(generated) for i in range(1000): x_pred = np.zeros((1, maxlen, len(chars))) for t, char in enumerate(sentence): x_pred[0, t, char_indices[char]] = 1. preds = model.predict(x_pred, verbose=0)[0] next_index = sample(preds, diversity) next_char = indices_char[next_index] generated += next_char sentence = sentence[1:] + next_char sys.stdout.write(next_char) sys.stdout.flush() print() . -- diversity: 0.5 - Hi, welcome back. I&#39;m certainly glad you can do this black canvas. I have the same clouds that the light on that little bushes that lives on the brush, and I&#39;m gonna go up in here. There, something like that. There, and we&#39;ll just put a little bit of this but of the Prussian blue to think on the brush here. We&#39;ll just push in some little bushes. And I wanna see what you looks like that, let&#39;s go back into the bright red. And you can make it a little bit of the little bushes and sidight to have a little bit of the little light color. Just a little bit of the background color to the colors on the brush, and I wanna do is in the background, I&#39;m gonna put a little bit of black in here and there. Just sort of lay the color. There, that easy. And we can see it in a little more of the lighter and they go right into the one of the lay of the paintings that you have the colors that you go. And we got a little bit of lighter on the canvas on the canvas, and we can see the sun up and make it any signes that come back in the color on . As you can see above, we&#39;ve generated a good amount of text from all of our transcriptions. Notice, that the model was able to understand color names (Prussian blue) and you kind of get the idea that its a story about painting. .",
            "url": "https://tonyhung.xyz/nlp/keras/2020/11/06/Bob-Ross-Episode-Generator.html",
            "relUrl": "/nlp/keras/2020/11/06/Bob-Ross-Episode-Generator.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://tonyhung.xyz/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tonyhung.xyz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tonyhung.xyz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}