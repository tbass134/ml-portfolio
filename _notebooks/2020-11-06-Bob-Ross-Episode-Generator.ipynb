{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bob Ross Episode Text Generator\n",
    "> The following shows how to create a text generator using LSTM's in Keras.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [nlp, keras]"
   ]
  },
  {
   "source": [
    "This project shows how we can gather data and build a model to generate text in the style of bob ross. \n",
    "In order to gather data, we'll be using a script called [download-yt-playlist.py](bob_ross/scripts/download-yt-playlist.py) that uses the YouTube API to download a Bob Ross playlist. This playlist contains most of the Bob Ross epiodes as well as the transcript from each epiode\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mIXC6MsAvpMN",
    "outputId": "a8097efa-2aa2-4599-aaea-e9f2200c0197",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/c7/741c97d7366f4779ca73d244904978b43a81fd37d85fcf05ad19d472c1ce/beautifulsoup4-4.6.3-py2-none-any.whl\n",
      "Installing collected packages: beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXW5JmmAjlSF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the dataset that we created using the `download-ty-playlist` script,\n",
    "The csv is included in the repo \n",
    "\n",
    "we'll then load the dataset into a pandas dataframe\n",
    "Our csv contains 249 rows, which are the number of episodes that was returned by the script.\n",
    "We've removed any columns that are empty, since not all of the episodes had a transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "colab_type": "code",
    "id": "pQY4xTOWoe4s",
    "outputId": "459a3ca6-a20b-48b8-b492-93ba01f50974",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet.publishedAt</th>\n",
       "      <th>snippet.title</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snippet.description</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Season 21 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Valley View, Tranquil Dawn, Royal Majesty, Serenity, Cabin at Trails End, Mountain Rhapsody, Wilderness Cabin, By the Sea, Indian Summer, Blue Winter, Desert Glow, Lone Mountain, and Floridaâ€™s Glory.\\n\\nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe\\n\\nSeason 21 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi5_UcEWkQZu23WzQP1Tkxq3\\n\\nThe Joy of Painting Season 20 is now on iTunes! - http://bit.ly/iTunesBobRoss\\n\\nOfficial Bob Ross website - http://www.BobRoss.com\\n\\nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss\\n\\nAll episodes of Bob Ross are now live on Roku - http://bit.ly/BobRossOnRoku\\n\\nOriginally aired on 9/5/1990</th>\n",
       "      <td>2015-03-25 16:32:35</td>\n",
       "      <td>Bob Ross - Valley View (Season 21 Episode 1)</td>\n",
       "      <td>(bright music) - Hello, I&amp;#39;m Bob Ross and\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Season 6 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Blue River, Nature's Edge, Morning Mist, Whispering Stream, Secluded Forest, Snow Trail, Arctic Beauty, Horizons West, High Chateau, Country Life, Western Expanse, Marshlands, and Blaze of Color.\\n\\nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe\\n\\nSeason 6 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi5UR35RJsvL0Xvlm3oeY4Ma\\n\\nThe Joy of Painting : Season 20 is now on iTunes! http://bit.ly/iTunesBobRoss\\n\\nOfficial Bob Ross website - http://www.BobRoss.com\\n\\nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss</th>\n",
       "      <td>2015-03-27 17:01:20</td>\n",
       "      <td>Bob Ross - Arctic Beauty (Season 6 Episode 7)</td>\n",
       "      <td>- Welcome back. Awful glad you could join me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Season 6 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Blue River, Nature's Edge, Morning Mist, Whispering Stream, Secluded Forest, Snow Trail, Arctic Beauty, Horizons West, High Chateau, Country Life, Western Expanse, Marshlands, and Blaze of Color.\\n\\nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe\\n\\nSeason 6 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi5UR35RJsvL0Xvlm3oeY4Ma\\n\\nThe Joy of Painting : Season 20 is now on iTunes! http://bit.ly/iTunesBobRoss\\n\\nOfficial Bob Ross website - http://www.BobRoss.com\\n\\nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss</th>\n",
       "      <td>2015-03-27 17:24:24</td>\n",
       "      <td>Bob Ross - Horizons West (Season 6 Episode 8)</td>\n",
       "      <td>- Welcome back, I&amp;#39;m awful\\nglad to see you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Season 6 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Blue River, Nature's Edge, Morning Mist, Whispering Stream, Secluded Forest, Snow Trail, Arctic Beauty, Horizons West, High Chateau, Country Life, Western Expanse, Marshlands, and Blaze of Color.\\n\\nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe\\n\\nSeason 6 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi5UR35RJsvL0Xvlm3oeY4Ma\\n\\nThe Joy of Painting : Season 20 is now on iTunes! http://bit.ly/iTunesBobRoss\\n\\nOfficial Bob Ross website - http://www.BobRoss.com\\n\\nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss</th>\n",
       "      <td>2015-03-27 18:16:39</td>\n",
       "      <td>Bob Ross - Blue River (Season 6 Episode 1)</td>\n",
       "      <td>(peaceful instrumental music) - Hello, I&amp;#39;m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Season 5 of The Joy of Painting with Bob Ross features the following wonderful painting instructions: Mountain Waterfall, Twilight Meadow, Mountain Blossoms, Winter Stillness, Quiet Pond, OCean Sunrise, Bubbling Brook, Arizona Splendor, Anatomy of a Wave, The Windmill, Autumn Glory, Indian Girl, and Meadow Stream.\\n\\nSubscribe to the official Bob Ross YouTube channel - http://bit.ly/BobRossSubscribe\\n\\nSeason 5 Playlist: https://www.youtube.com/playlist?list=PLAEQD0ULngi6bAFRfcqgpKP4T4SnoxoAz\\n\\nThe Joy of Painting : Season 20 is now on iTunes! http://bit.ly/iTunesBobRoss\\n\\nOfficial Bob Ross website - http://www.BobRoss.com\\n\\nOfficial Bob Ross Twitch.tv Stream! - http://twitch.tv/BobRoss</th>\n",
       "      <td>2015-03-27 18:34:49</td>\n",
       "      <td>Bob Ross - Twilight Meadow (Season 5 Episode 2)</td>\n",
       "      <td>- Hi, welcome back. I&amp;#39;m glad to see you to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   snippet.publishedAt  \\\n",
       "snippet.description                                                      \n",
       "Season 21 of The Joy of Painting with Bob Ross ... 2015-03-25 16:32:35   \n",
       "Season 6 of The Joy of Painting with Bob Ross f... 2015-03-27 17:01:20   \n",
       "Season 6 of The Joy of Painting with Bob Ross f... 2015-03-27 17:24:24   \n",
       "Season 6 of The Joy of Painting with Bob Ross f... 2015-03-27 18:16:39   \n",
       "Season 5 of The Joy of Painting with Bob Ross f... 2015-03-27 18:34:49   \n",
       "\n",
       "                                                                                      snippet.title  \\\n",
       "snippet.description                                                                                   \n",
       "Season 21 of The Joy of Painting with Bob Ross ...     Bob Ross - Valley View (Season 21 Episode 1)   \n",
       "Season 6 of The Joy of Painting with Bob Ross f...    Bob Ross - Arctic Beauty (Season 6 Episode 7)   \n",
       "Season 6 of The Joy of Painting with Bob Ross f...    Bob Ross - Horizons West (Season 6 Episode 8)   \n",
       "Season 6 of The Joy of Painting with Bob Ross f...       Bob Ross - Blue River (Season 6 Episode 1)   \n",
       "Season 5 of The Joy of Painting with Bob Ross f...  Bob Ross - Twilight Meadow (Season 5 Episode 2)   \n",
       "\n",
       "                                                                                           transcript  \n",
       "snippet.description                                                                                    \n",
       "Season 21 of The Joy of Painting with Bob Ross ...  (bright music) - Hello, I&#39;m Bob Ross and\\n...  \n",
       "Season 6 of The Joy of Painting with Bob Ross f...  - Welcome back. Awful glad you could join me t...  \n",
       "Season 6 of The Joy of Painting with Bob Ross f...  - Welcome back, I&#39;m awful\\nglad to see you...  \n",
       "Season 6 of The Joy of Painting with Bob Ross f...  (peaceful instrumental music) - Hello, I&#39;m...  \n",
       "Season 5 of The Joy of Painting with Bob Ross f...  - Hi, welcome back. I&#39;m glad to see you to...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bob_ross/bob_ross_episodes.csv', index_col=0, parse_dates=['snippet.publishedAt'], usecols=['snippet.description', 'snippet.publishedAt', 'snippet.title', 'transcript'])\n",
    "df.dropna(inplace=True)\n",
    "# df['snippet.publishedAt'] =pd.to_datetime(df['snippet.publishedAt'])\n",
    "df.sort_values(by='snippet.publishedAt', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will build out text generator.\n",
    "We'll do the following,\n",
    "- load a sample of the dataset (about 30%)\",\n",
    "- combine all the transcription into one long string,\n",
    "- We use BeautifulSoup to remove any html tags in the text,\n",
    "- we'll then generate a list of all the characters in the transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heCG2l5CqgU3"
   },
   "outputs": [],
   "source": [
    "#only use about %20 of rows\n",
    "test_df = df.sample(frac=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ubg0biyv0YNc",
    "outputId": "32a2f48b-7e46-4810-8564-326a666fe4d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlaLE9GsuNKM"
   },
   "outputs": [],
   "source": [
    "#combine transcription into 1 list\n",
    "descriptions = ''\n",
    "all_transcriptions = ''\n",
    "for index, row in test_df.iterrows():\n",
    "    all_transcriptions += BeautifulSoup(row['transcript'],\"lxml\").get_text().replace('\\n', ' ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9UOqLMf4y-Ap",
    "outputId": "2a0cbeff-b99c-484c-a8d6-1d19e1999a6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1522162"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_transcriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll just display a piece of the all_transcriptions just to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LnMuX2ARy_f4",
    "outputId": "bf474070-4b5c-410f-a836-0642e43d000e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- Hi, welcome back. I'm certainly glad you could join us today. And, as you can see, today I have on\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_transcriptions[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gACp3W9-0ioi",
    "outputId": "693709ec-60d8-4a26-feb9-18ed1d6ee99a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique characters (i.e., features): 81\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(all_transcriptions)))\n",
    "print('Count of unique characters (i.e., features):', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll generate seperate lists of all the strings that we'll feed into the model\n",
    "This list is 40 charcters of the full text, seperated by 3 characters(`step`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "HVwbQooP0zo3",
    "outputId": "3dcb628e-ae7c-4a10-c555-a95c2a866113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 507374\n",
      "[\"- Hi, welcome back. I'm certainly glad y\", \"i, welcome back. I'm certainly glad you \", \"welcome back. I'm certainly glad you cou\", \"come back. I'm certainly glad you could \", \"e back. I'm certainly glad you could joi\", \"ack. I'm certainly glad you could join u\", \". I'm certainly glad you could join us t\", \"'m certainly glad you could join us toda\", 'certainly glad you could join us today. ', 'tainly glad you could join us today. And'] \n",
      "\n",
      "['o', 'c', 'l', 'j', 'n', 's', 'o', 'y', 'A', ',']\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(all_transcriptions) - maxlen, step):\n",
    "    sentences.append(all_transcriptions[i: i + maxlen])\n",
    "    next_chars.append(all_transcriptions[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print(sentences[:10], \"\\n\")\n",
    "print(next_chars[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 507374 lists, that each contain 40 characters of the string,\n",
    "The first list is `- Hi, welcome back. I'm certainly glad y`, followed by `i, welcome back. I'm certainly glad you`\n",
    "\n",
    "Next, we'll create tensors of x and y, that contain the lists of all the sentences, we've created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6RfCipg1Ng3"
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builing The Model\n",
    "\n",
    "Next, we'll build out our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O5QS6ekk3x91",
    "outputId": "01a49d8f-2014-413b-b8df-95431eb877a3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are 2 functions that will print the prediction from each epoch, as well as the `temperature`\n",
    "\n",
    "temperature is defined as the following:\n",
    "\n",
    "\"Temperature is a scaling factor applied to the outputs of our dense layer before applying the softmaxactivation function. In a nutshell, it defines how conservative or creative the model's guesses are for the next character in a sequence. Lower values of temperature (e.g., 0.2) will generate \\\"safe\\\" guesses whereas values of temperature above 1.0 will start to generate riskier guesses. Think of it as the amount of surpise you'd have at seeing an English word start with \\\"st\\\" versus \\\"sg\\\". When temperature is low, we may get lots of the's and and's; when temperature is high, things get more unpredictable.\n",
    "\n",
    "-- https://medium.freecodecamp.org/applied-introduction-to-lstms-for-text-generation-380158b29fb3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lir9Q5Bv4s-y"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked for specified epochs. Prints generated text.\n",
    "    # Using epoch+1 to be consistent with the training epochs printed by Keras\n",
    "    if epoch+1 == 1 or epoch+1 == 15:\n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(all_transcriptions) - maxlen - 1)\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = all_transcriptions[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "    else:\n",
    "        print()\n",
    "        print('----- Not generating text after Epoch: %d' % epoch)\n",
    "\n",
    "generate_text = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_basic_model()\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(batch_size, input_shape=(maxlen,len(chars))))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll create our model.\n",
    "After a few tests, i've seen that having 2 LSTMs with a batch size of 256, returns very good results.\n",
    "The first model is a basic model with 1 LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1765
    },
    "colab_type": "code",
    "id": "XfpmAHbm6Gzz",
    "outputId": "ea46a2da-0372-49c6-fdda-3c6aad97ac73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " - 168s - loss: 1.4472\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ainted in your mountain. There. Isn't th\"\n",
      "ainted in your mountain. There. Isn't the back, and there we go the to start out of the back. There we go with the back, and there. There we go. There. There. There. There. And we'll go back and start the background of this one of the back of the base of the back. There. And we'll start one of the brush on the back, and there's a little bit of the back. There we go that one of the back. There we go the back. And we want the back, and th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ainted in your mountain. There. Isn't th\"\n",
      "ainted in your mountain. There. Isn't that easy like that. So let's do that. So let's go the back. Where we got the way of the old bright over the touching. I have to make the let the hang here. I stay over there, and that's run on the base the ingict of right in here lives right there. Okay, let's just go back and so back and just tap a little bright on our we want that with the lings on the old bit of the backer. Maybe they're wark it\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ainted in your mountain. There. Isn't th\"\n",
      "ainted in your mountain. There. Isn't them, ay up a many, back come spung it up ther hal chane and illoss, grettle strong, let's haf hereon well refcacalions and we just do clims to you're leanto so wet. There. There's and here and that easy. Hever in that's good let that little magic you. They's a little 'bounder There. There. And we use a littled, down like right here. And we can darker this tell. Don't kich xop and darked. Now the up\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ainted in your mountain. There. Isn't th\"\n",
      "ainted in your mountain. There. Isn't that you one DaneDrly. Othat'els, bat that time. Let's do kap lives loanince wherever we just bring that lives rignt down here. Okayh over this paintings in ther staday shader, you, if a dirdiciodd then only shapes oir all right. There, they're nice that big,. E. There we goess like the painting and a little paintings. Changen and beons that you'res. So nothill shay, fan brost, he ndime sortmort pea\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.44717, saving model to weights.hdf5\n",
      "Epoch 2/15\n",
      " - 165s - loss: 1.1462\n",
      "\n",
      "----- Not generating text after Epoch: 1\n",
      "\n",
      "Epoch 00002: loss improved from 1.44717 to 1.14621, saving model to weights.hdf5\n",
      "Epoch 3/15\n",
      " - 165s - loss: 1.0918\n",
      "\n",
      "----- Not generating text after Epoch: 2\n",
      "\n",
      "Epoch 00003: loss improved from 1.14621 to 1.09180, saving model to weights.hdf5\n",
      "Epoch 4/15\n",
      " - 164s - loss: 1.0639\n",
      "\n",
      "----- Not generating text after Epoch: 3\n",
      "\n",
      "Epoch 00004: loss improved from 1.09180 to 1.06386, saving model to weights.hdf5\n",
      "Epoch 5/15\n",
      " - 166s - loss: 1.0443\n",
      "\n",
      "----- Not generating text after Epoch: 4\n",
      "\n",
      "Epoch 00005: loss improved from 1.06386 to 1.04430, saving model to weights.hdf5\n",
      "Epoch 6/15\n",
      " - 165s - loss: 1.0308\n",
      "\n",
      "----- Not generating text after Epoch: 5\n",
      "\n",
      "Epoch 00006: loss improved from 1.04430 to 1.03085, saving model to weights.hdf5\n",
      "Epoch 7/15\n",
      " - 165s - loss: 1.0200\n",
      "\n",
      "----- Not generating text after Epoch: 6\n",
      "\n",
      "Epoch 00007: loss improved from 1.03085 to 1.02002, saving model to weights.hdf5\n",
      "Epoch 8/15\n",
      " - 163s - loss: 1.0105\n",
      "\n",
      "----- Not generating text after Epoch: 7\n",
      "\n",
      "Epoch 00008: loss improved from 1.02002 to 1.01048, saving model to weights.hdf5\n",
      "Epoch 9/15\n",
      " - 165s - loss: 1.0039\n",
      "\n",
      "----- Not generating text after Epoch: 8\n",
      "\n",
      "Epoch 00009: loss improved from 1.01048 to 1.00394, saving model to weights.hdf5\n",
      "Epoch 10/15\n",
      " - 167s - loss: 0.9996\n",
      "\n",
      "----- Not generating text after Epoch: 9\n",
      "\n",
      "Epoch 00010: loss improved from 1.00394 to 0.99955, saving model to weights.hdf5\n",
      "Epoch 11/15\n",
      " - 162s - loss: 0.9939\n",
      "\n",
      "----- Not generating text after Epoch: 10\n",
      "\n",
      "Epoch 00011: loss improved from 0.99955 to 0.99389, saving model to weights.hdf5\n",
      "Epoch 12/15\n",
      " - 166s - loss: 0.9918\n",
      "\n",
      "----- Not generating text after Epoch: 11\n",
      "\n",
      "Epoch 00012: loss improved from 0.99389 to 0.99178, saving model to weights.hdf5\n",
      "Epoch 13/15\n",
      " - 166s - loss: 0.9900\n",
      "\n",
      "----- Not generating text after Epoch: 12\n",
      "\n",
      "Epoch 00013: loss improved from 0.99178 to 0.98998, saving model to weights.hdf5\n",
      "Epoch 14/15\n",
      " - 165s - loss: 0.9873\n",
      "\n",
      "----- Not generating text after Epoch: 13\n",
      "\n",
      "Epoch 00014: loss improved from 0.98998 to 0.98731, saving model to weights.hdf5\n",
      "Epoch 15/15\n",
      " - 165s - loss: 0.9886\n",
      "\n",
      "----- Generating text after Epoch: 14\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"tly tap this and pull down at the same t\"\n",
      "tly tap this and pull down at the same tore of the brush and the secret. And the"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y're back here in the bright and that lives right there. There we go. And it's all the color in the bristles that live in the bright and little bit of the bright red of the brush. And they come right there. And they're a little bit of the brown and some little things that live out here. And a little bit of the bright and then we have a little bit of the bit \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"tly tap this and pull down at the same t\"\n",
      "tly tap this and pull down at the same thing because that's where a little bit of color in there. And we'll go sid it to get into the top, and they come right there. That still the black gesso basic little black, they paint thinner, right over the brush and happening beautiful way of your painting, that live out there. And becommen it back and sort of some big tree that lives real and sort of this side it dark to dry the big mountches a\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"tly tap this and pull down at the same t\"\n",
      "tly tap this and pull down at the same toret it. Now then? I wanna have another little trees very shows here and there it'll put very all sort of decision it. Okay, and you's hadmy nice little, glad you want over here. They looks like dark afthainctoujbly aftosistic oary, maybe right in here, but that's goingtilly to dry pressort of the oh, that's sone's. , just to diffue all the would becamina's at sion. Let's start over here on cere a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"tly tap this and pull down at the same t\"\n",
      "tly tap this and pull down at the same twig, sup. some little grassy area, little workent, you see, it is gings. Don't neem comrstece a. Inlistion, makes this just in tood a reasinn forward. oDanatided just a knife.  is us how a little foackes using aapriaens. I think we see letta after we very, it's undistant green something. It ...  as phintione into a phthalo, eve all right forward, uh it, tllay right over, . Just bring ae. Anothe. O\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.98731\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = build_basic_model()\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# fit model using our gpu\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              verbose=1,\n",
    "              callbacks=[generate_text, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can see that the results were good, but lets go deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCuLGdXI6Rfz"
   },
   "source": [
    "## Builing a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll be using 2 LSTM's and dropout, durning training, we'll save the best model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "batch_size=256\n",
    "learning_rate = 0.01\n",
    "def build_deeper_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(batch_size, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(batch_size))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "    \n",
    "model = build_deeper_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"bob_ross/weights-deepeer.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# fit model using our gpu\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y,\n",
    "              batch_size=64,\n",
    "              epochs=15,\n",
    "              verbose=1,\n",
    "              callbacks=[generate_text, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, which took about 2 hours to train, using a GCP instance with a Tesla P100 GPU, we load the best model and perfrom a prediction\n",
    "\n",
    "We loaded our model from our weights, and now we can predict\n",
    "I choose a temperature of `0.5`. it seemed the have the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f78bb4f7748>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_weights(\"weights-deepeer.hdf5\")\n",
    "from keras.models import load_model\n",
    "model = load_model(\"bob_ross/weights-deepeer.hdf5\")\n",
    "model\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- diversity: 0.5\n",
      "- Hi, welcome back. I'm certainly glad you can do this black canvas. I have the same clouds that the light on that little bushes that lives on the brush, and I'm gonna go up in here. There, something like that. There, and we'll just put a little bit of this but of the Prussian blue to think on the brush here. We'll just push in some little bushes. And I wanna see what you looks like that, let's go back into the bright red. And you can make it a little bit of the little bushes and sidight to have a little bit of the little light color. Just a little bit of the background color to the colors on the brush, and I wanna do is in the background, I'm gonna put a little bit of black in here and there. Just sort of lay the color. There, that easy. And we can see it in a little more of the lighter and they go right into the one of the lay of the paintings that you have the colors that you go. And we got a little bit of lighter on the canvas on the canvas, and we can see the sun up and make it any signes that come back in the color on \n"
     ]
    }
   ],
   "source": [
    "start_index = 0\n",
    "\n",
    "\n",
    "for diversity in [0.5]:\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = all_transcriptions[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "#     print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(1000):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we've generated a good amount of text from all of our transcriptions.\n",
    "Notice, that the model was able to understand color names (Prussian blue) and you kind of get the idea that its a story about painting.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bob Ross Episode Generator.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}