{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS Spam Classifier - Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DKu0dKb1gwDR",
        "6-IXlfF48OYj",
        "UNFvFlfWmdIE",
        "R4Mj7wg4gmbS"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMlFr97+apgpXI2bLdlw1v8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building a Machine Learning model to detect spam in SMS\n",
        "> Building a machine learing model to predict that a SMS messages is spam or not\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [jupyter]\n",
        "- image: images/chart-preview.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFZk-hPFVEh0"
      },
      "source": [
        "In this notebook, we'll show how to build a simple machine learning model to predict that a SMS is spam or not. \n",
        "\n",
        "The notebook was built to go along with my talk in May 2020 for [Vonage Developer Day](https://www.vonage.com/about-us/vonage-stories/vonage-developer-day/)\n",
        "\n",
        "youtube: https://www.youtube.com/watch?v=5d4_HpMLXf4&t=1s\n",
        "\n",
        "We'll be using the scikit-learn library to train a model on a set of messages which are labeled as spam and non spam(aka ham) messages. \n",
        "\n",
        "After our model is trained, we'll deploy to an AWS Lambda in which its input will be a message, and its output will be the prediction(spam or ham).\n",
        "\n",
        "Before we build a model, we'll need some data. So we'll use the [SMS Spam Collection DataSet](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection).\n",
        "\n",
        "This dataset contains over 5k messages which are labeled spam or ham.\n",
        "In the following cell, we'll download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbVXInvFPsYx",
        "outputId": "c91adeb5-ab61-473b-d168-0293e7534971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!wget --no-check-certificate https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "!unzip /content/smsspamcollection.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-22 00:49:18--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203415 (199K) [application/x-httpd-php]\n",
            "Saving to: ‘smsspamcollection.zip’\n",
            "\n",
            "smsspamcollection.z 100%[===================>] 198.65K   509KB/s    in 0.4s    \n",
            "\n",
            "2020-07-22 00:49:19 (509 KB/s) - ‘smsspamcollection.zip’ saved [203415/203415]\n",
            "\n",
            "Archive:  /content/smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkm8tqwXTWz2"
      },
      "source": [
        "Once we have downloaded the datatset, we'll load into a Pandas Dataframe and view the first 10 rows of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc1fvT-tQR2o",
        "outputId": "b6f3e284-2812-470d-bcb0-b6d297d63f36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/SMSSpamCollection\", sep='\\t', header=None, names=['label', 'message'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AFlrj4JT26R"
      },
      "source": [
        "Next, we need to first understand the data before building a model.\n",
        "\n",
        "We'll first need to see how many messages are considered spam or ham"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEEscYZSQqRr",
        "outputId": "0582d4d4-0560-4fb3-d58e-c2335a0d8cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA0lpp9wUKfC"
      },
      "source": [
        "From the cell above, we see that 4825 messages are valid messages, and only 747 messages are labled as spam.\n",
        "\n",
        "Lets now just view some messages that are ham and some that are spam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xzBYJs6UkQe",
        "outputId": "9176b96d-62a1-468c-be14-2b350519c46c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "spam = df[df[\"label\"] == \"spam\"]\n",
        "spam.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>spam</td>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>spam</td>\n",
              "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                            message\n",
              "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "5   spam  FreeMsg Hey there darling it's been 3 week's n...\n",
              "8   spam  WINNER!! As a valued network customer you have...\n",
              "9   spam  Had your mobile 11 months or more? U R entitle...\n",
              "11  spam  SIX chances to win CASH! From 100 to 20,000 po..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc0GZ0oKR-Fc",
        "outputId": "64de5ed4-59d7-4b3e-f820-6041c4e1720a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "ham = df[df[\"label\"] == \"ham\"]\n",
        "ham.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
              "6   ham  Even my brother is not like to speak with me. ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9a3bPKXWNfz"
      },
      "source": [
        "after looking at some messages that spam and ham, we can see the spam messages look spammy.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E48Jw01bx6Ri"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYcT8uQ4Wd1B"
      },
      "source": [
        "The next step is to get the dataset ready to build a model. A machine learning model can only deal with numbers, so we'll have to convert our text into numbers using `TfidfVectorizer`\n",
        "\n",
        "TfidfVectorizer converts a collection of raw documents to a matrix of [term frequency-inverse document frequency](http://www.tfidf.com/) features. Also known as TF-IDF.\n",
        "\n",
        "In our case, a document is each message. For each message, we'll compute the number of times a term is in our document divied by all the terms in the document times the total number of documents divded by the number of documents that contain the specific term\n",
        "\n",
        "![](https://miro.medium.com/max/1066/1*eIDZG3Ot5DP8SKXAvBVALQ.png)\n",
        "[source](https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd)\n",
        "\n",
        "The output will be a matrix in which the rows will be all the terms, and the colums will be all the documents\n",
        "![](https://miro.medium.com/max/1400/1*n4s0LZS1Qi46pF3aaYzE0A.png)\n",
        "\n",
        "[This notebook by Mike Bernico](https://github.com/mbernico/CS570/blob/master/module_1/TFIDF.ipynb) by goes into more detail on TF-IDF and how to calucate without using sklearn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8iK2yd6yauW"
      },
      "source": [
        "first, we'll split the dataset into a train and test set. For the training set, we'll take 80% of the data from the dataset, and use that for training the model. The rest of the dataset(20%) will be used for testing the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY5fl2_KcF3n"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size = 0.2, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXwVXb6zzDwg"
      },
      "source": [
        "once we split our data, we can use the TfidfVectorizer. This will return a sparse matrix(a matrix with mostly 0's)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XcNFXR9fqpX"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DLD6dzSwe7q"
      },
      "source": [
        "After we fit the TfidfVectorizer to the sentenes, lets plot the matrix as a pandas dataframe to understand what TfidfVectorizer is doing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k30aIRxKoZ6W",
        "outputId": "7c73364d-fbe0-4f1a-b8d5-ee175b240406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "feature_names = vectorizer.get_feature_names()\n",
        "tfid_df = pd.DataFrame(tfs.T.todense(), index=feature_names)\n",
        "print(tfid_df[1200:1205])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           0     1     2         3     4     ...  4452  4453  4454  4455  4456\n",
            "backdoor    0.0   0.0   0.0  0.000000   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "backwards   0.0   0.0   0.0  0.000000   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "bad         0.0   0.0   0.0  0.193352   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "badass      0.0   0.0   0.0  0.000000   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "badly       0.0   0.0   0.0  0.000000   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[5 rows x 4457 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATo-UBFYwxP7"
      },
      "source": [
        "From the table above, each word in our dataset are the rows are the sentenes index are the columns. We've only plotted a few rows in the middle of the dataframe for a better understanding of the data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4zdOuj34AZ4"
      },
      "source": [
        "Next, we'll train a model using Gaussian Naive Bayes in scikit-learn. Its a good starting algorithm for text classification. We'll then print out the accuracy of the model by using the training set and our confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgQWSBygyEYi"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHQ8hTVG7tif"
      },
      "source": [
        "To train our model, we'll use A Navie Bayes algorhtymn to train our model\n",
        "\n",
        "The formula for Navie Bayes is:\n",
        "\\\\[ P(S|W) = P(W|S) \\times P(S) \\over P(W|S) \\times P(S) + P(W|H) \\times P(h) \\\\].\n",
        "\n",
        "**P(s|w)**  - The probability(**P**) of a message is spam(**s**) Given(**|**) a word(**w**)\n",
        "\n",
        "**=**\n",
        "\n",
        "**P(w|s)**   - probability(**P**) that a word(**w**) is spam(**s**)\n",
        "\n",
        "*\n",
        "\n",
        "**P(s)** - Overall probability(**P**) that ANY message is spam(**s**)\n",
        "\n",
        "**/**\n",
        "\n",
        "**P(w|s)** - probability(**P**) that a word(**w**) exists in spam messages(**s**)\n",
        "\n",
        "*\n",
        "\n",
        "**P(s)** - Overall probability(**P**) that ANY message is spam(**s**)\n",
        "\n",
        "**+**\n",
        "\n",
        "**P(w|h)** - probability(**P**) the word(**w**) appears in non-spam(**h**) messages\n",
        "\n",
        "*\n",
        "\n",
        "**P(h)** - Overall probability(**P**) that any message is not-spam(**h**)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w34jwTOehvOw",
        "outputId": "65304073-9985-4607-f10e-9fe45104fa45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train.toarray(),y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffofI_-U7PAs",
        "outputId": "5c85a34e-fd4c-46c2-a2b2-7fa78f324ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_true, y_pred = y_test, clf.predict(X_test.toarray())\n",
        "accuracy_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8986547085201794"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUgUqfPW7VRB",
        "outputId": "a4948903-b4b3-441b-85a0-18e7253b8925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.89      0.94       968\n",
            "        spam       0.57      0.93      0.71       147\n",
            "\n",
            "    accuracy                           0.90      1115\n",
            "   macro avg       0.78      0.91      0.82      1115\n",
            "weighted avg       0.93      0.90      0.91      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2OuiJzA8XpD",
        "outputId": "de12911a-7137-4bdd-fc8e-2572fe84f853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "cmtx = pd.DataFrame(\n",
        "    confusion_matrix(y_true, y_pred, labels=['ham', 'spam']), \n",
        "    index=['ham', 'spam'], \n",
        "    columns=['ham', 'spam']\n",
        ")\n",
        "print(cmtx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      ham  spam\n",
            "ham   866   102\n",
            "spam   11   136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw3ybc6s9SbE"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7HXCFxQ9UIS",
        "outputId": "7d9f6831-b1e9-463c-93a5-e23cc6a5b9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {\"var_smoothing\":[1e-9, 1e-5, 1e-1]}\n",
        "gs_clf = GridSearchCV(\n",
        "        GaussianNB(), parameters)\n",
        "gs_clf.fit(X_train.toarray(),y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=GaussianNB(priors=None, var_smoothing=1e-09),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'var_smoothing': [1e-09, 1e-05, 0.1]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGYbivbY-KZ5",
        "outputId": "df05a36a-582f-4999-bf61-91320db7181e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gs_clf.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'var_smoothing': 0.1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBEke9AP-P3V",
        "outputId": "cf40af0b-204e-4a29-eca8-af002b301b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_true, y_pred = y_test, gs_clf.predict(X_test.toarray())\n",
        "accuracy_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9650224215246637"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idngcag3B82Z",
        "outputId": "20b2e7cc-fbc0-4115-bd6b-ca6431658854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "cmtx = pd.DataFrame(\n",
        "    confusion_matrix(y_true, y_pred, labels=['ham', 'spam']), \n",
        "    index=['ham', 'spam'], \n",
        "    columns=['ham', 'spam']\n",
        ")\n",
        "print(cmtx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      ham  spam\n",
            "ham   932    36\n",
            "spam    3   144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpP5Vl4zGMtG",
        "outputId": "88a161a2-8592-45f2-8c86-3a1bf2522076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.96      0.98       968\n",
            "        spam       0.80      0.98      0.88       147\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.90      0.97      0.93      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTEV9V46wJ4m"
      },
      "source": [
        "From our trained model, we get about 96% accuracy. Which is pretty good. \n",
        "\n",
        "We also print out the confusion_matrix. This shows how many messages were classificed correctly. In the first column and first row, we see that 866 messages that were classified as ham were actaully ham and 136 messages that were predicted as spam, were in fact spam.\n",
        "\n",
        "Next, lets test our model with some examples messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bps5RGkkyIxP"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynHt3Qltoflg",
        "outputId": "ef75825a-6eb7-4b18-fe39-e8ceb7c12bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "message = vectorizer.transform([\"i'm on my way home\"])\n",
        "message = message.toarray()\n",
        "gs_clf.predict(message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ham'], dtype='<U4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls-hzhOUo96o",
        "outputId": "b0c0b837-fe28-4ebd-bd6f-4952b531801c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "message = vectorizer.transform([\"this offer is to good to be true\"])\n",
        "message = message.toarray()\n",
        "gs_clf.predict(message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['spam'], dtype='<U4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_ZJgOvY41N8"
      },
      "source": [
        "The final step is the save the model and the tf-idf vectorizer. We will use these when clasifing incoming messages on our lambda function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxLsrlhZ-L5N",
        "outputId": "af7a692e-28fb-4503-e385-617c94dc7adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import joblib\n",
        "joblib.dump(gs_clf, \"model.pkl\")\n",
        "joblib.dump(vectorizer, \"vectorizer.pkl\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vectorizer.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKu0dKb1gwDR"
      },
      "source": [
        "# Lambda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X8TJzBn8-zp"
      },
      "source": [
        "Once our model is trained, we'll now put it in a production envioroment.\n",
        "\n",
        "For this example, we'll create a lambda function to host our model.\n",
        "\n",
        "The lambda function will be attached to an API gateway in which we'll be able to have a endpoint to make our predictions\n",
        "\n",
        "Deploying a scikit-learn model to lambda isnt as easy as you would think. You can't just import your libraries, espcially scikit-learn to work.\n",
        "\n",
        "Here's what we'll need to do in order to deploy our model\n",
        "* Spin up EC2 instance\n",
        "* SSH into the instance and install our dependencies\n",
        "* copy the lambda function code from this [repo](https://github.com/tbass134/SMS-Spam-Classifier-lambda)\n",
        "* Run a bash script that zips up the :\n",
        "* zip the code, including the packages\n",
        "* upload to S3\n",
        "* point the lambda function to to s3 file\n",
        "\n",
        "## Create an EC2 instance\n",
        "If you have an aws account:\n",
        "* Go to EC2 on the console and click `Launch Instance`.\n",
        "* Select the first available AMI(Amazon Linux 2 AMI). \n",
        "* Select the t2.micro instance, then click `Review and Launch`\n",
        "* Click the Next button\n",
        "* Under IAM Role, Click Create New Role\n",
        "* Create a new role with the following policies:\n",
        "  AmazonS3FullAccess\n",
        "  AWSLambdaFullAccess\n",
        "  Name your role and click create role\n",
        "* Under permissions, create a new role that has access to the following:\n",
        "* lambda full access\n",
        "* S3 full access\n",
        "\n",
        "These will be needed when uploading our code to your S3 bucket and pointing the lambda function to zip file that will be creating later.\n",
        "\n",
        "* Create a new private key pair and click `Lanuch Instance`\n",
        "* Note, in order to use the key, you have to run `chmod 400` on the key when downloaded to your local machine.\n",
        "\n",
        "\n",
        "After the instance spins up, you'll need to connect to it via ssh\n",
        "* Find the newly created instance on EC2 and click `Connect`\n",
        "* On your local machine, navigate to terminal and run the the command from the Example. It will look something like:\n",
        "```bash\n",
        "ssh -i \"{PATH TO KEY}\" {user_name}@ec2-{instance_ip}.compute-1.amazonaws.com\n",
        "```\n",
        "\n",
        "## Install packages\n",
        "Before installing packages, you will need to install python and pip. You can follow the steps [here](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install-linux.html)\n",
        "These will most likey be:\n",
        "```bash \n",
        "sudo yum install python37\n",
        "curl -O https://bootstrap.pypa.io/get-pip.py\n",
        " python3 get-pip.py --user\n",
        " verify pip is installed using\n",
        " ```bash\n",
        " pip --version\n",
        " ```\n",
        " You will also need to install git\n",
        " ```bash\n",
        " sudo yum install git -y\n",
        " ```\n",
        " When connected to the instance, clone the repo\n",
        "```bash\n",
        "git clone https://github.com/tbass134/SMS-Spam-Classifier-lambda\n",
        "```\n",
        "This repo contains everything we need to make predictions. These includes the pickle files from the model and vectorizer, as well as the lambda function to make predictions and returns its response\n",
        "cd into the SMS-Spam-Classifier-lambda/lambda folder\n",
        "* Next, you you will need to install the `sklearn` library.\n",
        "* On your instance, type:\n",
        "`pip install -t . sklearn`\n",
        "This will import the library into its own folder\n",
        "\n",
        "\n",
        "Next, if you want to use your trained model, it will need to be uploaded into your ec2 instance. \n",
        "If your using Google Colab, navigate to the files tab, right click on `my_model.pk` and `vectorizer.pkl` and click download.\n",
        "Note, the sample repo already contains a trained model so this is optional.\n",
        "\n",
        "To upload your trained model, you can use a few ways:\n",
        " * Fork the repo, add your models, and checkout on the ec2 instance\n",
        "  You can use `scp` to copy to files from your local machine to the instance\n",
        "  To upload the model file we saved\n",
        "  ```bash\n",
        "  scp -i {PATH_TO_KEY} vectorizer.pkl ec2-user@{INSTANCE_NAME}:\n",
        "  ```\n",
        "\n",
        "  and we'll do the same for the model\n",
        "  ```bash\n",
        "  scp -i {PATH_TO_KEY} my_model.pkl ec2-user@{INSTANCE_NAME}:\n",
        "  ```\n",
        "\n",
        "* The other method is to upload the files to s3 and have your lambda function load the files from there using Boto\n",
        "```Python\n",
        "  def load_s3_file(key):\n",
        "      obj = s3.Object(MODEL_BUCKET, key)\n",
        "      body = obj.get()['Body'].read()\n",
        "      return joblib.load(BytesIO(body))   \n",
        "\n",
        "  model = load_s3_file({PATH_TO_S3_MODEL}\n",
        "  vectorizer = load_s3_file({PATH_TO_S3_VECTORIZER}\n",
        "```\n",
        "\n",
        "\n",
        "## Create lambda function\n",
        "* On the AWS console, navigate to https://console.aws.amazon.com/lambda\n",
        "* Click on the Create function button\n",
        "* Make sure `Author from scratch` is selected\n",
        "* Name your function\n",
        "* Set the runtime to Python 3.7\n",
        "* Under Execution Role, create a new role with basic permissions\n",
        "* Click `Create Function`\n",
        "\n",
        "## Create S3 bucket\n",
        "In order to push our code to a lambda function, we need to first copy zip up the code and libraies to a S3 bucket. \n",
        "From here, our lambda function will load the zip file from this bucket.\n",
        "* On the AWS console under `Services`, Search for `S3`\n",
        "* Click `Create Bucket`\n",
        "* Name your bucket, and click Create Bucket at the bottom of the page.\n",
        "\n",
        "\n",
        "## Upload to lambda\n",
        "Next, we'll run the `publish.sh`script inside the root of the repo, which does the following:\n",
        "* zip up the pacakages, including our Python code, model and transformer.\n",
        "* upload the zip to an S3 bucket\n",
        "* point our lambda function to this bucket\n",
        "\n",
        "when calling this script, we need to pass in 3 arguments:\n",
        "* The name of the zip file. We can call it `zip.zip` for now\n",
        "* The name of the S3 bucket that we will upload the zip to\n",
        "* the name of lambda function \n",
        "```bash\n",
        "bash publish.sh {ZIP_FILE_NAME} {S3_BUCKET} {LAMBDA_FUNCTION_NAME}\n",
        "```\n",
        "\n",
        "If everything is successful, your lambda function will be deployed. \n",
        "If you see errors, make sure your EC2 instance has a IAM role that has an S3 permission, and Lambda permissions.\n",
        "See this [guide](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html) for more info.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-IXlfF48OYj"
      },
      "source": [
        "## Add HTTP endpoint\n",
        "The final piece will be to add a API gateway.\n",
        "On the configuration tab on the lambda function\n",
        "* click `Add Trigger`\n",
        "* Click on the select a trigger box and select `API Gateway`\n",
        "* Click on `Create an API`\n",
        "* Set API Type to `REST API`\n",
        "* Set Security to `OPEN` (make sure to secure when deploying for production)\n",
        "* At the bottom, click `Add`\n",
        "\n",
        "For detail, see this [documentation](https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-lambda.html#api-as-lambda-proxy-create-api-resources)\n",
        "\n",
        "We can now test the endpoint by using curl and making a call to our endpoint.\n",
        "Under `API Gateway` section in lambda, click on oi\n",
        "\n",
        "In the lambda function, we are looking for the `message` GET parameter. When we make our request, we'll pass a query parameter called `message`. This will contain the string we want to make a prediction on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPqx7d7Vm2sz"
      },
      "source": [
        "ham_message = \"im on my way home\".replace(\" \", \"%20\")\n",
        "ham_message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6s4UFcKsSE8",
        "outputId": "6a604951-b135-4b78-dc0b-61eae06b79f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%bash -s \"$ham_message\"\n",
        "curl --location --request GET \"https://e18fmcospk.execute-api.us-east-1.amazonaws.com/default/spam-detection?message=$1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"prediction\": \"ham\"}"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r100    21  100    21    0     0     10      0  0:00:02  0:00:02 --:--:--    10\r100    21  100    21    0     0     10      0  0:00:02  0:00:02 --:--:--    10\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PfebuZHs8tK"
      },
      "source": [
        "spam_message = \"this offer is to good to be true\".replace(\" \", \"%20\")\n",
        "spam_message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jFy3GIPtcRX",
        "outputId": "063ed503-898a-4088-bab8-88a208b0ecd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%bash -s \"$spam_message\"\n",
        "curl --location --request GET \"https://e18fmcospk.execute-api.us-east-1.amazonaws.com/default/spam-detection?message=$1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"prediction\": \"spam\"}"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r100    22  100    22    0     0     13      0  0:00:01  0:00:01 --:--:--    13\r100    22  100    22    0     0     13      0  0:00:01  0:00:01 --:--:--    13\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doTNiDTt5IHh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1fo9i-Z5HKw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtZ5zly1o7oh"
      },
      "source": [
        "# Google Cloud Functions\n",
        "\n",
        "For non-amazon users, we can use Google Cloud Functions to deploy our model for use in our Vonage SMS API app\n",
        "![](https://pbs.twimg.com/media/EdYKMFDUwAAwj0T?format=png&name=large)\n",
        "Code is [here](https://gist.github.com/tbass134/7985c0adf44c938d6e683c18dabac8f9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNFvFlfWmdIE"
      },
      "source": [
        "# Create Vonage SMS Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3hi0A7GMV2p"
      },
      "source": [
        "The final step is to build a Vonage SMS Application.\n",
        "Have a look at this blog post on how to build yourself\n",
        "Our application will receive an SMS\n",
        "https://developer.nexmo.com/messaging/sms/code-snippets/receiving-an-sms\n",
        "\n",
        "and will send a SMS back to the user with its prediction\n",
        "https://developer.nexmo.com/messaging/sms/code-snippets/send-an-sms\n",
        "\n",
        "<img src=\"https://i.ibb.co/8mxfBKW/IMG-9-BA66209-F969-1.png\" alt=\"drawing\" width=\"300\" text-align=\"center\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvhqwKkhYRxu"
      },
      "source": [
        "To work through this example, you will need the following\n",
        "* Login / Signup to [Vonage SMS API](https://dashboard.nexmo.com/sign-up)\n",
        "* Rent a phone number\n",
        "* Assign a publicly accessable url via [ngrok](https://www.nexmo.com/blog/2017/07/04/local-development-nexmo-ngrok-tunnel-dr) to that phone number\n",
        "\n",
        "We'll also build a simple Flask app that will make a request to our API Gateway\n",
        "```bash\n",
        "git clone https://github.com/tbass134/SMS-Spam-Classifier-lambda.git\n",
        "cd app\n",
        "```\n",
        "\n",
        "Next we'll create a virtual environment and install the requirements using pip\n",
        "```bash\n",
        "virtualenv venv --python=python3\n",
        "source venv/bin/activate\n",
        "pip install -r requirments.txt\n",
        "```\n",
        "\n",
        "Next, create a `.env` file with the following:\n",
        "```bash\n",
        "NEXMO_API_KEY={YOUR_NEXMO_API_KEY}\n",
        "NEXMO_API_SECRET={YOUR_NEXMO_API_SECRET}\n",
        "NEXMO_NUMBER={YOUR_NEXMO_NUMBER\n",
        "API_GATEWAY_URL={FULL_API_GATEWAY}\n",
        "```\n",
        "\n",
        "Finally, you can run the application:\n",
        "```bash\n",
        "python app.py\n",
        "```\n",
        "This will spin up a webserver listening on PORT 3000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Mj7wg4gmbS"
      },
      "source": [
        "# Fin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5qgAatWgnwl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}